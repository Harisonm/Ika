{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /opt/conda/envs/envPython37/lib/python3.7/site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/conda/envs/envPython37/lib/python3.7/site-packages (from BeautifulSoup4) (1.9.2)\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys \n",
    "!pip install BeautifulSoup4\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")  # Download text data sets, including stop words\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"french\"))\n",
    "from past.builtins import xrange\n",
    "import numpy as np\n",
    "import collections\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/manitra_mails.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_to_words(raw_body):\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = raw_body.lower().split()                             \n",
    "    #\n",
    "    # In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"french\"))     \n",
    "    # \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"body\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(0,num_reviews):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append(body_to_words(str(train[\"body\"][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set mail body...\n",
      "\n",
      "body 1000 of 2664\n",
      "\n",
      "body 2000 of 2664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the training set mail body...\\n\")\n",
    "clean_train_reviews = []\n",
    "for i in range(0,num_reviews):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0):\n",
    "        print(\"body %d of %d\\n\" % ( i+1,num_reviews))                                                                   \n",
    "    clean_train_reviews.append(body_to_words(str(train[\"body\"][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for body in clean_train_reviews:\n",
    "#    print(body[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features from a Bag of Words (Using scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \\\n",
    "                             tokenizer = None, \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None, \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2664, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "#For each, print the vocabulary word and the number of times it \n",
    "#appears in the training set\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "        #tokenizes and stems the text\n",
    "        tokens = word_tokenize(text)\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('french')]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sentences(sentences, nb_of_clusters=5):\n",
    "            tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                            stop_words=stopwords.words('french'),\n",
    "                                            max_df=0.9,\n",
    "                                            min_df=0.1,\n",
    "                                            lowercase=False)\n",
    "            #builds a tf-idf matrix for the sentences\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "            kmeans = KMeans(n_clusters=nb_of_clusters)\n",
    "            kmeans.fit(tfidf_matrix)\n",
    "            clusters = collections.defaultdict(list)\n",
    "            for i, label in enumerate(kmeans.labels_):\n",
    "                    clusters[label].append(i)\n",
    "            return dict(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "nclusters= 5\n",
    "clusters = cluster_sentences(clean_train_reviews, nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "#terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=5)\n",
    "gmm.fit(train_data_features)\n",
    "proba_lists = gmm.predict_proba(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "colored_arrays = np.matrix(proba_lists)\n",
    "colored_tuples = [tuple(i.tolist()[0]) for i in colored_arrays]\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "ax.scatter(train_data_features[:, 3], train_data_features[:, 0], train_data_features[:, 2],\n",
    "          c=colored_tuples, edgecolor=\"k\", s=50)\n",
    "ax.set_xlabel(\"Petal width\")\n",
    "ax.set_ylabel(\"Sepal length\")\n",
    "ax.set_zlabel(\"Petal length\")\n",
    "plt.title(\"Gaussian Mixture Model\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans\n",
    "km = KMeans(n_clusters=5)\n",
    "km.fit(train_data_features)\n",
    "km.predict(train_data_features)\n",
    "labels = km.labels_\n",
    "#Plotting\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "ax.scatter(train_data_features[:, 3], train_data_features[:, 0], train_data_features[:, 2],\n",
    "          c=labels.astype(np.float), edgecolor=\"k\", s=50)\n",
    "ax.set_xlabel(\"Petal width\")\n",
    "ax.set_ylabel(\"Sepal length\")\n",
    "ax.set_zlabel(\"Petal length\")\n",
    "plt.title(\"K Means\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envPython37)",
   "language": "python",
   "name": "envpython37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
