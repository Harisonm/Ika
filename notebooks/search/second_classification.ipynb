{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from nltk.corpus import stopwords  # Import the stop word list\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# WARNING : Nltk save download in your personal folder, don t forget delete them after run model.\n",
    "# Download text data sets, including stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def body_to_words(raw_body):\n",
    "    # Fonction permettant de convertir un commentaire brut en chaîne de mots\n",
    "    # L’entrée est une chaîne unique (mail de film brute), et\n",
    "    # la sortie est une chaîne unique (mail pré-traitée)\n",
    "    # Supprimer les non-lettres\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_body)\n",
    "\n",
    "    # Convertir en minuscules, diviser en mots individuels\n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # En Python, la recherche d'un ensemble est beaucoup plus rapide que la recherche\n",
    "    # une liste, convertissez les mots vides en un ensemble\n",
    "    stops = set(stopwords.words(\"french\"))\n",
    "\n",
    "    # Joindre les mots dans une chaîne séparée par un espace,\n",
    "    # et retourne le résultat.\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    return \" \".join(meaningful_words)\n",
    "\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('french')]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def cluster_sentences(sentences, nb_of_clusters=2):\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                       stop_words=stopwords.words('french'),\n",
    "                                       max_df=0.9,\n",
    "                                       min_df=0.1,\n",
    "                                       lowercase=False)\n",
    "    # Construit une matrice tf-idf pour les phrases\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    kmeans = KMeans(n_clusters=nb_of_clusters,\n",
    "                    init='k-means++',\n",
    "                    max_iter=300,\n",
    "                    n_init=1)\n",
    "\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    print(\"ORDER CENTROIDS\",order_centroids)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    print(\"TERMS\", terms)\n",
    "    for i in range(nb_of_clusters):\n",
    "        print(\"Cluster %d:\" % i),\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(\"%s\"% terms[ind])\n",
    "    \n",
    "    clusters = collections.defaultdict(list)\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        print(kmeans.labels_)\n",
    "        clusters[label].append(i)\n",
    "    return dict(clusters)\n",
    "\n",
    "\n",
    "def k_means_model(train_data_features):\n",
    "    # KMeans\n",
    "    km = KMeans(n_clusters=5)\n",
    "    km.fit(train_data_features)\n",
    "    km.predict(train_data_features)\n",
    "    labels = km.labels_\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(1, figsize=(7, 7))\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "    ax.scatter(train_data_features[:, 3], train_data_features[:, 0], train_data_features[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor=\"k\", s=50)\n",
    "    ax.set_xlabel(\"ham width\")\n",
    "    ax.set_ylabel(\"spam length\")\n",
    "    ax.set_zlabel(\"ham length\")\n",
    "    plt.title(\"K Means\", fontsize=14)\n",
    "\n",
    "\n",
    "def gaussian_mixture_model(train_data_features):\n",
    "    # Gaussian Mixture Model\n",
    "    gmm = GaussianMixture(n_components=5)\n",
    "    gmm.fit(train_data_features)\n",
    "    proba_lists = gmm.predict_proba(train_data_features)\n",
    "\n",
    "    # Plotting\n",
    "    colored_arrays = np.matrix(proba_lists)\n",
    "    colored_tuples = [tuple(i.tolist()[0]) for i in colored_arrays]\n",
    "    fig = plt.figure(1, figsize=(7, 7))\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "    ax.scatter(train_data_features[:, 3], train_data_features[:, 0], train_data_features[:, 2],\n",
    "               c=colored_tuples, edgecolor=\"k\", s=50)\n",
    "    ax.set_xlabel(\"ham width\")\n",
    "    ax.set_ylabel(\"spam length\")\n",
    "    ax.set_zlabel(\"hamp length\")\n",
    "    plt.title(\"Gaussian Mixture Model\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---MAIN----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './dataset/manitra_mails.csv'\n",
    "train = pd.read_csv(data, encoding='utf-8')\n",
    "\n",
    "# Obtenez le nombre de mail en fonction de la taille de la colonne dataframe\n",
    "num_reviews = train[\"body\"].size\n",
    "\n",
    "# Initialise une liste vide pour contenir les mails\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Boucle sur chaque mail; créer un index i qui va de 0 à la longueur du Nombre de mail\n",
    "for i in range(0, num_reviews):\n",
    "    # Appelez notre fonction pour chacun et ajoutez le résultat à la liste des mails prétraité\n",
    "    clean_train_reviews.append(body_to_words(str(train[\"body\"][i])))\n",
    "\n",
    "print(\"Cleaning and parsing the training set mail body...\\n\")\n",
    "clean_train_reviews = []\n",
    "for i in range(0, num_reviews):\n",
    "\n",
    "    # Si l’index est divisible par 1000, affiche un message.\n",
    "    if ((i + 1) % 1000 == 0):\n",
    "        print(\"body %d of %d\\n\" % (i + 1, num_reviews))\n",
    "    clean_train_reviews.append(body_to_words(str(train[\"body\"][i])))\n",
    "\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "# Initialise l'objet \"CountVectorizer\", qui est celui de scikit-learn outil de sac de mots. \n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=5000)\n",
    "\n",
    "# fit_transform () remplit deux fonctions: tout d’abord, cela correspond au modèle\n",
    "# et apprend le vocabulaire; deuxièmement, cela transforme nos données d'entraînement\n",
    "# en vecteurs de caractéristiques. L'entrée à fit_transform doit être une liste de\n",
    "# chaînes.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Les tableaux Numpy sont faciles à utiliser, convertissez donc le résultat en tableau\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# Résumer les comptes de chaque mot de vocabulaire\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# Clustering\n",
    "nclusters = 4\n",
    "clusters = cluster_sentences(clean_train_reviews, nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster in range(nclusters):\n",
    "#        print(\"cluster \", cluster, \":\")\n",
    "#        for i, sentence in enumerate(clusters[cluster]):\n",
    "#            print(\"\\tsentence \", i, \": \", clean_train_reviews[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_mixture_model(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_model(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envPython37)",
   "language": "python",
   "name": "envpython37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
