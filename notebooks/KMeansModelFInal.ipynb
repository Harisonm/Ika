{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords  # Import the stop word list\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from metrics import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = open(\"./resultat_clustering.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_WORDS = ['import', 'px', 'width', 'class', 'pad', 'none', 'td', 'height', 'tabl', 'font', 'px', 'font',\n",
    "                   'pad', 'margin', 'serif', 'helvetica', 'text', 'color', 'san', 'arial', 'read', 'your', 'thi',\n",
    "                   'with', 'data', 'learn', 'from', 'email', 'that', 'have', 'bonjour', 'plu', 'compt', 'cordial',\n",
    "                   'pari', 'mail', 'bien', 'tout', 'screen', 'content', 'don', 'imag', 'med', 'util', 'page',\n",
    "                   'aur', 'aurion', 'auron', 'avi', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'eûm',\n",
    "                   'fuss', 'fussion', 'fûm', 'mêm', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon',\n",
    "                   'votr', 'éti', 'étion', 'ête', 'dat', 'hav', 'helvetic', 'non', 'decor', 'lin', 'underlin', 'bord',\n",
    "                   'left', 'top', 'auto', 'display', 'padding', 'bottom', 'this', 'learning', 'about', 'mor', 'will',\n",
    "                   'siz', 'decor', 'max', 'right', 'a', 'externalclass', 'img', 'block', 'align', 'body,', 'remerc',\n",
    "                   'bon', 'merc', 'envoy', 'souhait', 'messag', 'fair', 'unsubscrib', 'mak', 'sent', 'help',\n",
    "                   'vis', 'part', 'amp', 'com', 'cod', 'plus', 'messag', 'cet', 'servic', 'tous',\n",
    "                   'inform', 'merc', 'pass', 'adress', 'souh', 'body', 'background', 'hov', 'span', 'styl', 'solid',\n",
    "                   'family', 'cliqu', 'lien', 'fr', 'consult', 'pouv', 'lign', 'jour', 'utilis', 'demand', 'veuill',\n",
    "                   'appliqu', 'don', 'chang', 'social', 'manitr', 'imag', 'min', 'only', 'equip', 'only', 'medi',\n",
    "                   'plac', 'question', 'recevoir', 'relat', 'repondr', 'repons', 'reserv', 'googl', 'view', 'tim',\n",
    "                   'aide', 'autre', 'bientot', 'cedex', 'cent', 'detail', 'direct', 'droit', 'espace', 'ete',\n",
    "                   'etre', 'foot', 'hide', 'inlin', 'link', 'mobil', 'outlook', 'site', 'sous', 'suivie',\n",
    "                   'trouve', 'weight', 'aid', 'autr', 'espac', 'hid', 'pag', 'repon', 'sit', 'style', 'time', 'use',\n",
    "                   'veuillez', 'visit', 'merci', 'more', 'outlin', 'place', 'pouvez', 'rel', 'securit', 'size',\n",
    "                   'size line', 'important', 'mcntextcontent',\n",
    "                   'suiv', 'trouv', 'what', 'you', 'non', ',', 'bodi', 'capit', 'etr', 'famili', 'helvet',\n",
    "                   'onli', 'sou', 'suivi', 'tou', 'utili', 'vi', 'applic', 'bonn', 'border', 'border border',\n",
    "                   'center', 'cett', 'cliquez', 'code', 'commun', 'concern', 'consultez', 'date',\n",
    "                   'float', 'footer', 'hover', 'line', 'make', 'media', 'souhaitez', 'ranaivoharison', 'manitra',\n",
    "                   'manitra ranaivoharison', 'francisco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_to_words(raw_body):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_body)\n",
    "    text = re.sub('<[^<]+?>', '', letters_only)\n",
    "    text_clean = ' '.join([w for w in text.split() if ((len(w) > 3) and (len(w) < 23))])\n",
    "    words = text_clean.lower().split()\n",
    "    stop_words = set(stopwords.words('french') + stopwords.words('english') + TOKENIZED_WORDS)\n",
    "    meaningful_words = [w for w in words if w not in stop_words]\n",
    "    # clean_words = [w for w in meaningful_words if w not in TOKENIZED_WORDS]\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in (stopwords.words('french') + stopwords.words('english'))]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in\n",
    "              nltk.sent_tokenize(text, language='french') for word in nltk.word_tokenize(sent, language='french')]\n",
    "    filtered_tokens = []\n",
    "    stemmer = SnowballStemmer(language='french')\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text, language='french') for word in\n",
    "              nltk.word_tokenize(sent, language='french')]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_dataset(dataset_train):\n",
    "    # Obtenez le nombre de mail en fonction de la taille de la colonne dataframe\n",
    "    num_reviews = dataset_train[\"body\"].size\n",
    "\n",
    "    # Initialise une liste vide pour contenir les mails\n",
    "    clean_train_reviews = []\n",
    "    for i in range(0, num_reviews):\n",
    "\n",
    "        # Si l’index est divisible par 1000, affiche un message.\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"body %d of %d\\n\" % (i + 1, num_reviews))\n",
    "        clean_train_reviews.append({'body': body_to_words(str(dataset_train[\"body\"][i])),\n",
    "                                    'idMail': dataset_train[\"idMail\"][i]})\n",
    "\n",
    "    print(\"Creating the bag of words...\\n\")\n",
    "    return clean_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clustering_group(k_means_model, tfidf_matrix):\n",
    "    cluster_labels = k_means_model.fit_predict(tfidf_matrix)\n",
    "    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "    sample_silhouette_values = silhouette_samples(tfidf_matrix, cluster_labels)\n",
    "    centers = k_means_model.cluster_centers_\n",
    "    n_clusters = centers.shape[0]\n",
    "    \n",
    "    #print('cluster_labels', cluster_labels)\n",
    "    #print('silhouette_avg', silhouette_avg)\n",
    "    #print('sample_silhouette_values', sample_silhouette_values)\n",
    "    #print('centers', centers)\n",
    "    #print('n_clusters', n_clusters)\n",
    "    return cluster_labels, silhouette_avg, sample_silhouette_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_details_cluster(vocab_frame, k_means_model, tfidf_matrix, tfidf_vectorizer, clusters, clean_train_reviews, n_clusters):\n",
    "    print(\"___________________________\")\n",
    "    order_centroids = k_means_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "    print(dist)\n",
    "\n",
    "    print('ORDER_CENTROIDS')\n",
    "    label = []\n",
    "    for cluster in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % cluster)\n",
    "        cluster_label = []\n",
    "        print(\"TERMS\", terms),\n",
    "        for ind in order_centroids[cluster, :n_clusters]:\n",
    "            print(\"%s\" % terms[ind])\n",
    "            label_name = vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore')\n",
    "            cluster_label.insert(cluster,label_name.decode('utf-8'))\n",
    "        label.append(cluster_label)\n",
    "    \n",
    "    print(label)\n",
    "        \n",
    "    for cluster in range(n_clusters):\n",
    "        FILE.write(\"cluster \" + str(cluster) + \":\" + \"\\n\")\n",
    "        FILE.write(\"centroid\" + str(cluster) + \"\\n\")\n",
    "        \n",
    "        for i, sentence in enumerate(clusters[cluster]):\n",
    "            clean_train_reviews[sentence]['cluster_group'] = str(cluster)\n",
    "            clean_train_reviews[sentence]['label'] = str(label[cluster])\n",
    "            # print(\"\\tsentence \", i, \": \", clean_train_reviews[sentence])\n",
    "            FILE.write(\"mail :\" + str(i) + \": \" + str(clean_train_reviews[sentence]) + \"\\n\")\n",
    "\n",
    "    \n",
    "    print(\"___________________________ \\n\")\n",
    "    centers = k_means_model.cluster_centers_\n",
    "\n",
    "    print('SCORE %s \\n', k_means_model.score(tfidf_matrix))\n",
    "    print('INERTIA %s \\n', k_means_model.inertia_)\n",
    "    print(\"CENTRE : %s\", centers)\n",
    "    print(\"TERMS\", terms)\n",
    "    return order_centroids, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_from_model(n_clusters, tfidf_matrix):\n",
    "    k_means_model = KMeans(n_clusters=n_clusters,\n",
    "                           init='k-means++',\n",
    "                           max_iter=300,\n",
    "                           n_init=1)\n",
    "\n",
    "    k_means_model.fit(tfidf_matrix)\n",
    "\n",
    "    clusters = collections.defaultdict(list)\n",
    "    for i, label in enumerate(k_means_model.labels_):\n",
    "        # print(k_means_model.labels_)\n",
    "        clusters[label].append(i)\n",
    "\n",
    "    return dict(clusters), k_means_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_matrix_vector(dataset):\n",
    "    train_body = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        train_body.append(dataset[i]['body'])\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem,\n",
    "                                       analyzer='word',\n",
    "                                       stop_words=stopwords.words('french') +\n",
    "                                                  TOKENIZED_WORDS +\n",
    "                                                  stopwords.words('english'),\n",
    "                                       max_df=0.8,\n",
    "                                       min_df=0.15,\n",
    "                                       lowercase=False,\n",
    "                                       use_idf=True,\n",
    "                                       max_features=200000,\n",
    "                                       ngram_range=(1, 3))\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(train_body)\n",
    "    print(tfidf_matrix.shape)\n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_to_words_lda(raw_body):\n",
    "    text_data = []\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_body)\n",
    "    text = re.sub('<[^<]+?>', '', letters_only)\n",
    "    text_clean = ' '.join([w for w in text.split() if ((len(w) > 3) and (len(w) < 23))])\n",
    "    words = text_clean.lower().split()\n",
    "    stop_words = set(stopwords.words('french') + stopwords.words('english') + TOKENIZED_WORDS)\n",
    "    meaningful_words = [w for w in words if w not in stop_words]\n",
    "    text_data.append(meaningful_words)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_dataset_lda(dataset_train):\n",
    "    # Obtenez le nombre de mail en fonction de la taille de la colonne dataframe\n",
    "    num_reviews = dataset_train[\"body\"].size\n",
    "\n",
    "    # Initialise une liste vide pour contenir les mails\n",
    "    clean_train_reviews = []\n",
    "    for i in range(0, num_reviews):\n",
    "\n",
    "        # Si l’index est divisible par 1000, affiche un message.\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"body %d of %d\\n\" % (i + 1, num_reviews))\n",
    "        clean_train_reviews.append({'body': body_to_words_lda(str(dataset_train[\"body\"][i])),\n",
    "                                    'idMail': dataset_train[\"idMail\"][i]})\n",
    "    \n",
    "    print(\"Creating the bag of words...\\n\")\n",
    "    return clean_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH ='./dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'mada.apps.creation@gmail.com1563099050.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH + data, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews_lda = pre_processing_dataset_lda(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['souhaitons',\n",
       " 'mettre',\n",
       " 'plateforme',\n",
       " 'mobile',\n",
       " 'desktop',\n",
       " 'client',\n",
       " 'interne',\n",
       " 'souhaite',\n",
       " 'mettre',\n",
       " 'contact',\n",
       " 'parler',\n",
       " 'besoins',\n",
       " 'projet',\n",
       " 'mamisoa']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews_lda[0]['body'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = [d['body'] for d in clean_train_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-f7afdb310122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'body.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dictionary.gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(body, open('body.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in body:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(totalvocab_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix, tfidf_vectorizer = build_tfidf_matrix_vector(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the within clusters sum-of-squares for 19 cluster amounts\n",
    "sum_of_squares = calculate_wcss(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the optimal number of clusters\n",
    "n_clusters = optimal_number_of_clusters(sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, k_means_model = build_cluster_from_model(n_clusters, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels, silhouette_avg, sample_silhouette_values = predict_clustering_group(k_means_model, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids, centers = show_details_cluster(vocab_frame,k_means_model, tfidf_matrix, tfidf_vectorizer, clusters,\n",
    "                                                clean_train_reviews, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyValList = ['0']\n",
    "#expectedResult = [d['body'] for d in clean_train_reviews if d['cluster_group'] in keyValList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envPython37)",
   "language": "python",
   "name": "envpython37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
