{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /opt/conda/envs/envPython37/lib/python3.7/site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/conda/envs/envPython37/lib/python3.7/site-packages (from BeautifulSoup4) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys \n",
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Lautch if you are movie_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('gmail_transform_manitra.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idMail</th>\n",
       "      <th>threadId</th>\n",
       "      <th>historyId</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>labelIds</th>\n",
       "      <th>spam</th>\n",
       "      <th>body</th>\n",
       "      <th>mimeType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168ec85b7b5aa105</td>\n",
       "      <td>168ec85b7b5aa105</td>\n",
       "      <td>1922967</td>\n",
       "      <td>\"La Direction de l'ESGI\" &lt;alumni@communication...</td>\n",
       "      <td>&lt;manitra.harison@gmail.com&gt;</td>\n",
       "      <td>Thu, 14 Feb 2019 16:00:11 +0100 (CET)</td>\n",
       "      <td>IMPORTANT, STARRED, CATEGORY_UPDATES, INBOX</td>\n",
       "      <td>0</td>\n",
       "      <td>Gala de Remise des Diplômes du Réseau GES - Pr...</td>\n",
       "      <td>text/plain;text/html;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1681730b78219cb5</td>\n",
       "      <td>1681730b78219cb5</td>\n",
       "      <td>1816318</td>\n",
       "      <td>Apple &lt;no_reply@email.apple.com&gt;</td>\n",
       "      <td>manitra.harison@gmail.com</td>\n",
       "      <td>Fri, 4 Jan 2019 04:48:14 +0000 (GMT)</td>\n",
       "      <td>UNREAD, IMPORTANT, CATEGORY_UPDATES</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple Facture --------------------------------...</td>\n",
       "      <td>text/plain;text/html;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1632029f7f36f331</td>\n",
       "      <td>1632029f7f36f331</td>\n",
       "      <td>1451163</td>\n",
       "      <td>Manitra Harison &lt;manitra.harison@gmail.com&gt;</td>\n",
       "      <td>Manitra Harison &lt;manitra.harison@gmail.com&gt;, S...</td>\n",
       "      <td>Wed, 2 May 2018 11:23:31 +0200</td>\n",
       "      <td>SENT, INBOX</td>\n",
       "      <td>0</td>\n",
       "      <td>c'est ça ton projet ??  -corpus.com/blog/metie...</td>\n",
       "      <td>text/plain;text/html;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1678969ed7aea8f2</td>\n",
       "      <td>1678969ed7aea8f2</td>\n",
       "      <td>1773023</td>\n",
       "      <td>Kamal HENNOU &lt;direction@esgi.fr&gt;</td>\n",
       "      <td>manitra.harison@gmail.com</td>\n",
       "      <td>Fri, 7 Dec 2018 15:12:37 +0100 (CET)</td>\n",
       "      <td>UNREAD, CATEGORY_PERSONAL, INBOX</td>\n",
       "      <td>0</td>\n",
       "      <td>Chers étudiant(e)s,  Depuis 2013, le concours ...</td>\n",
       "      <td>text/plain;text/html;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163a83dc97e02cf9</td>\n",
       "      <td>163a83dc97e02cf9</td>\n",
       "      <td>1497076</td>\n",
       "      <td>Glassdoor &lt;noreply@glassdoor.com&gt;</td>\n",
       "      <td>manitra.harison@gmail.com</td>\n",
       "      <td>Mon, 28 May 2018 19:33:30 +0000 (UTC)</td>\n",
       "      <td>STARRED, CATEGORY_UPDATES, INBOX</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>text/plain;text/html;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             idMail          threadId  historyId  \\\n",
       "0  168ec85b7b5aa105  168ec85b7b5aa105    1922967   \n",
       "1  1681730b78219cb5  1681730b78219cb5    1816318   \n",
       "2  1632029f7f36f331  1632029f7f36f331    1451163   \n",
       "3  1678969ed7aea8f2  1678969ed7aea8f2    1773023   \n",
       "4  163a83dc97e02cf9  163a83dc97e02cf9    1497076   \n",
       "\n",
       "                                                from  \\\n",
       "0  \"La Direction de l'ESGI\" <alumni@communication...   \n",
       "1                   Apple <no_reply@email.apple.com>   \n",
       "2        Manitra Harison <manitra.harison@gmail.com>   \n",
       "3                   Kamal HENNOU <direction@esgi.fr>   \n",
       "4                  Glassdoor <noreply@glassdoor.com>   \n",
       "\n",
       "                                                  to  \\\n",
       "0                        <manitra.harison@gmail.com>   \n",
       "1                          manitra.harison@gmail.com   \n",
       "2  Manitra Harison <manitra.harison@gmail.com>, S...   \n",
       "3                          manitra.harison@gmail.com   \n",
       "4                          manitra.harison@gmail.com   \n",
       "\n",
       "                                    date  \\\n",
       "0  Thu, 14 Feb 2019 16:00:11 +0100 (CET)   \n",
       "1   Fri, 4 Jan 2019 04:48:14 +0000 (GMT)   \n",
       "2         Wed, 2 May 2018 11:23:31 +0200   \n",
       "3   Fri, 7 Dec 2018 15:12:37 +0100 (CET)   \n",
       "4  Mon, 28 May 2018 19:33:30 +0000 (UTC)   \n",
       "\n",
       "                                      labelIds  spam  \\\n",
       "0  IMPORTANT, STARRED, CATEGORY_UPDATES, INBOX     0   \n",
       "1          UNREAD, IMPORTANT, CATEGORY_UPDATES     0   \n",
       "2                                  SENT, INBOX     0   \n",
       "3             UNREAD, CATEGORY_PERSONAL, INBOX     0   \n",
       "4             STARRED, CATEGORY_UPDATES, INBOX     0   \n",
       "\n",
       "                                                body               mimeType  \n",
       "0  Gala de Remise des Diplômes du Réseau GES - Pr...  text/plain;text/html;  \n",
       "1  Apple Facture --------------------------------...  text/plain;text/html;  \n",
       "2  c'est ça ton projet ??  -corpus.com/blog/metie...  text/plain;text/html;  \n",
       "3  Chers étudiant(e)s,  Depuis 2013, le concours ...  text/plain;text/html;  \n",
       "4                                                ...  text/plain;text/html;  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 10)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['idMail', 'threadId', 'historyId', 'from', 'to', 'date',\n",
       "       'labelIds', 'spam', 'body', 'mimeType'], dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gala de Remise des Diplômes du Réseau GES - Promotion 2017-2018 Consultez la version en ligne.: -&amp;c=gF69zLZR9TbcSFSsHWzh8Q73661970423464278572-188516  Cérémonie de remise des diplômes - GALA - Promotion 2017-2018 - SAMEDI 23 MARS 2019   Cher(e)s Diplômé(e)s,  L'équipe de l'ESGI a le plaisir de vous inviter et vos parents à votre Gala de remise des Diplômes le :   SAMEDI 23 MARS 2019 A la salle Wagram 39-41 avenue de Wagram, 75017 Paris Métro 1 Charles de Gaulles Etoile, Métro 2 Ternes TENUE CORRECTE EXIGÉE   Remise des diplômes Bachelors et Mastères ESGI :  Rendez-vous à 17h00, la cérémonie prendra fin vers 20h00.  Remise des diplômes de 17h00 à 17h45 (à partir de 17h45 vous ne pourrez plus retirer votre diplôme, vous devrez le récupérer au service des anciens à compter du Lundi 6 Mai).  Attention, ce mail n'est pas une confirmation de votre diplomation. Vous recevrez très prochainement par courrier la décision du jury.  La cérémonie n'est accessible qu'aux diplômés validés par le jury.  Pour retirer votre diplôme*, vous devez obligatoirement remplir l'enquête d'insertion en ligne avant le Mardi 19 Mars. Je confirme ma participation et réponds à l'enquête d'insertion  -VRBYweta34iyoUbVww8XHMtXw&amp;c=gF69zLZR9TbcSFSsHWzh8Q73661970423464278572-188516 Cette enquête est importante pour la reconnaissance de votre diplôme auprès des entreprises et pour renforcer la notoriété de votre formation. Elle ne vous prendra que 2 minutes.  Pour raison de sécurité, vous ne pouvez venir qu'avec deux accompagnants.  Nous espérons vous voir nombreux au Gala et sommes ravis de vous retrouver pour partager ce moment fort et unique !  La Direction de l'ESGI  Pour plus d'informations, contactez le service des anciens : anciens@reseau-ges.fr: mailto:anciens@reseau-ges.fr   Attention, ce mail n'est pas une confirmation de votre diplomation. Vous recevrez très prochainement par courrier la décision du jury.  *(sous réserve de validation du TOEIC ou autres pré-requis académiques, à défaut votre diplôme ne pourra vous être remis, seule une attestation de réussite provisoire vous sera délivrée)  Réseau GES\n",
      "Gala de Remise des Diplômes du Réseau GES - Promotion 2017-2018 Consultez la version en ligne.: -&c=gF69zLZR9TbcSFSsHWzh8Q73661970423464278572-188516  Cérémonie de remise des diplômes - GALA - Promotion 2017-2018 - SAMEDI 23 MARS 2019   Cher(e)s Diplômé(e)s,  L'équipe de l'ESGI a le plaisir de vous inviter et vos parents à votre Gala de remise des Diplômes le :   SAMEDI 23 MARS 2019 A la salle Wagram 39-41 avenue de Wagram, 75017 Paris Métro 1 Charles de Gaulles Etoile, Métro 2 Ternes TENUE CORRECTE EXIGÉE   Remise des diplômes Bachelors et Mastères ESGI :  Rendez-vous à 17h00, la cérémonie prendra fin vers 20h00.  Remise des diplômes de 17h00 à 17h45 (à partir de 17h45 vous ne pourrez plus retirer votre diplôme, vous devrez le récupérer au service des anciens à compter du Lundi 6 Mai).  Attention, ce mail n'est pas une confirmation de votre diplomation. Vous recevrez très prochainement par courrier la décision du jury.  La cérémonie n'est accessible qu'aux diplômés validés par le jury.  Pour retirer votre diplôme*, vous devez obligatoirement remplir l'enquête d'insertion en ligne avant le Mardi 19 Mars. Je confirme ma participation et réponds à l'enquête d'insertion  -VRBYweta34iyoUbVww8XHMtXw&c=gF69zLZR9TbcSFSsHWzh8Q73661970423464278572-188516 Cette enquête est importante pour la reconnaissance de votre diplôme auprès des entreprises et pour renforcer la notoriété de votre formation. Elle ne vous prendra que 2 minutes.  Pour raison de sécurité, vous ne pouvez venir qu'avec deux accompagnants.  Nous espérons vous voir nombreux au Gala et sommes ravis de vous retrouver pour partager ce moment fort et unique !  La Direction de l'ESGI  Pour plus d'informations, contactez le service des anciens : anciens@reseau-ges.fr: mailto:anciens@reseau-ges.fr   Attention, ce mail n'est pas une confirmation de votre diplomation. Vous recevrez très prochainement par courrier la décision du jury.  *(sous réserve de validation du TOEIC ou autres pré-requis académiques, à défaut votre diplôme ne pourra vous être remis, seule une attestation de réussite provisoire vous sera délivrée)  Réseau GES\n"
     ]
    }
   ],
   "source": [
    "# Import BeautifulSoup into your workspace\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "# Initialize the BeautifulSoup object on a single movie review     \n",
    "example1 = BeautifulSoup(train[\"body\"][0])  \n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "print(train[\"body\"][0])\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gala de Remise des Dipl mes du R seau GES   Promotion           Consultez la version en ligne     c gF  zLZR TbcSFSsHWzh Q                             C r monie de remise des dipl mes   GALA   Promotion             SAMEDI    MARS        Cher e s Dipl m  e s   L  quipe de l ESGI a le plaisir de vous inviter et vos parents   votre Gala de remise des Dipl mes le     SAMEDI    MARS      A la salle Wagram       avenue de Wagram        Paris M tro   Charles de Gaulles Etoile  M tro   Ternes TENUE CORRECTE EXIG E   Remise des dipl mes Bachelors et Mast res ESGI    Rendez vous     h    la c r monie prendra fin vers   h     Remise des dipl mes de   h       h      partir de   h   vous ne pourrez plus retirer votre dipl me  vous devrez le r cup rer au service des anciens   compter du Lundi   Mai    Attention  ce mail n est pas une confirmation de votre diplomation  Vous recevrez tr s prochainement par courrier la d cision du jury   La c r monie n est accessible qu aux dipl m s valid s par le jury   Pour retirer votre dipl me   vous devez obligatoirement remplir l enqu te d insertion en ligne avant le Mardi    Mars  Je confirme ma participation et r ponds   l enqu te d insertion   VRBYweta  iyoUbVww XHMtXw c gF  zLZR TbcSFSsHWzh Q                            Cette enqu te est importante pour la reconnaissance de votre dipl me aupr s des entreprises et pour renforcer la notori t  de votre formation  Elle ne vous prendra que   minutes   Pour raison de s curit   vous ne pouvez venir qu avec deux accompagnants   Nous esp rons vous voir nombreux au Gala et sommes ravis de vous retrouver pour partager ce moment fort et unique    La Direction de l ESGI  Pour plus d informations  contactez le service des anciens   anciens reseau ges fr  mailto anciens reseau ges fr   Attention  ce mail n est pas une confirmation de votre diplomation  Vous recevrez tr s prochainement par courrier la d cision du jury     sous r serve de validation du TOEIC ou autres pr  requis acad miques    d faut votre dipl me ne pourra vous  tre remis  seule une attestation de r ussite provisoire vous sera d livr e   R seau GES\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")  # Download text data sets, including stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gala', 'remise', 'dipl', 'r', 'seau', 'ges', 'promotion', 'consultez', 'version', 'ligne', 'gf', 'zlzr', 'tbcsfsshwzh', 'q', 'r', 'monie', 'remise', 'dipl', 'gala', 'promotion', 'samedi', 'mars', 'cher', 'e', 'dipl', 'e', 'quipe', 'esgi', 'a', 'plaisir', 'inviter', 'parents', 'gala', 'remise', 'dipl', 'samedi', 'mars', 'a', 'salle', 'wagram', 'avenue', 'wagram', 'paris', 'tro', 'charles', 'gaulles', 'etoile', 'tro', 'ternes', 'tenue', 'correcte', 'exig', 'e', 'remise', 'dipl', 'bachelors', 'mast', 'res', 'esgi', 'rendez', 'h', 'r', 'monie', 'prendra', 'fin', 'vers', 'h', 'remise', 'dipl', 'h', 'h', 'partir', 'h', 'pourrez', 'plus', 'retirer', 'dipl', 'devrez', 'r', 'cup', 'rer', 'service', 'anciens', 'compter', 'lundi', 'mai', 'attention', 'mail', 'confirmation', 'diplomation', 'recevrez', 'tr', 'prochainement', 'courrier', 'cision', 'jury', 'r', 'monie', 'accessible', 'dipl', 'valid', 'jury', 'retirer', 'dipl', 'devez', 'obligatoirement', 'remplir', 'enqu', 'insertion', 'ligne', 'avant', 'mardi', 'mars', 'confirme', 'participation', 'r', 'ponds', 'enqu', 'insertion', 'vrbyweta', 'iyoubvww', 'xhmtxw', 'gf', 'zlzr', 'tbcsfsshwzh', 'q', 'cette', 'enqu', 'importante', 'reconnaissance', 'dipl', 'aupr', 'entreprises', 'renforcer', 'notori', 'formation', 'prendra', 'minutes', 'raison', 'curit', 'pouvez', 'venir', 'deux', 'accompagnants', 'esp', 'rons', 'voir', 'nombreux', 'gala', 'ravis', 'retrouver', 'partager', 'moment', 'fort', 'unique', 'direction', 'esgi', 'plus', 'informations', 'contactez', 'service', 'anciens', 'anciens', 'reseau', 'ges', 'fr', 'mailto', 'anciens', 'reseau', 'ges', 'fr', 'attention', 'mail', 'confirmation', 'diplomation', 'recevrez', 'tr', 'prochainement', 'courrier', 'cision', 'jury', 'sous', 'r', 'serve', 'validation', 'toeic', 'autres', 'pr', 'requis', 'acad', 'miques', 'faut', 'dipl', 'pourra', 'tre', 'remis', 'seule', 'attestation', 'r', 'ussite', 'provisoire', 'livr', 'e', 'r', 'seau', 'ges']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"french\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_to_words(raw_body):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_body) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"french\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gala remise dipl r seau ges promotion consultez version ligne amp gf zlzr tbcsfsshwzh q r monie remise dipl gala promotion samedi mars cher e dipl e quipe esgi a plaisir inviter parents gala remise dipl samedi mars a salle wagram avenue wagram paris tro charles gaulles etoile tro ternes tenue correcte exig e remise dipl bachelors mast res esgi rendez h r monie prendra fin vers h remise dipl h h partir h pourrez plus retirer dipl devrez r cup rer service anciens compter lundi mai attention mail confirmation diplomation recevrez tr prochainement courrier cision jury r monie accessible dipl valid jury retirer dipl devez obligatoirement remplir enqu insertion ligne avant mardi mars confirme participation r ponds enqu insertion vrbyweta iyoubvww xhmtxw amp gf zlzr tbcsfsshwzh q cette enqu importante reconnaissance dipl aupr entreprises renforcer notori formation prendra minutes raison curit pouvez venir deux accompagnants esp rons voir nombreux gala ravis retrouver partager moment fort unique direction esgi plus informations contactez service anciens anciens reseau ges fr mailto anciens reseau ges fr attention mail confirmation diplomation recevrez tr prochainement courrier cision jury sous r serve validation toeic autres pr requis acad miques faut dipl pourra tre remis seule attestation r ussite provisoire livr e r seau ges\n"
     ]
    }
   ],
   "source": [
    "clean_body = body_to_words(train[\"body\"][0])\n",
    "print(clean_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from past.builtins import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"body\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(0,num_reviews):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append(body_to_words(str(train[\"body\"][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set mail body...\n",
      "\n",
      "Review 1000 of 1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the training set mail body...\\n\")\n",
    "clean_train_reviews = []\n",
    "for i in range(0,num_reviews):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1,num_reviews))                                                                   \n",
    "    clean_train_reviews.append(body_to_words(str(train[\"body\"][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(clean_train_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features from a Bag of Words (Using scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \\\n",
    "                             tokenizer = None, \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None, \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-10db1b203a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import collections\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from pprint import pprint\n",
    "\n",
    "\n",
    "    def word_tokenizer(text):\n",
    "            #tokenizes and stems the text\n",
    "            tokens = word_tokenize(text)\n",
    "            stemmer = PorterStemmer()\n",
    "            tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('french')]\n",
    "            return tokens\n",
    "\n",
    "\n",
    "    def cluster_sentences(sentences, nb_of_clusters=5):\n",
    "            tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                            stop_words=stopwords.words('french'),\n",
    "                                            max_df=0.9,\n",
    "                                            min_df=0.1,\n",
    "                                            lowercase=False)\n",
    "            #builds a tf-idf matrix for the sentences\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "            kmeans = KMeans(n_clusters=nb_of_clusters)\n",
    "            kmeans.fit(tfidf_matrix)\n",
    "            clusters = collections.defaultdict(list)\n",
    "            for i, label in enumerate(kmeans.labels_):\n",
    "                    clusters[label].append(i)\n",
    "            return dict(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-74124bf0e7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnclusters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-165-6b2cc3a3085b>\u001b[0m in \u001b[0;36mcluster_sentences\u001b[0;34m(sentences, nb_of_clusters)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                         lowercase=False)\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#builds a tf-idf matrix for the sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_of_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \"\"\"\n\u001b[1;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-6b2cc3a3085b>\u001b[0m in \u001b[0;36mword_tokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#tokenizes and stems the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/envPython37/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "nclusters= 3\n",
    "clusters = cluster_sentences(train_data_features, nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1494, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-167-1a83f139ab21>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-167-1a83f139ab21>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    # print(count, tag)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    # print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envPython37)",
   "language": "python",
   "name": "envpython37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": "import pandas as pd\nimport os"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
